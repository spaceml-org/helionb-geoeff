{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92073643",
   "metadata": {},
   "source": [
    "# Ground magnetic perturbation forecasting\n",
    "\n",
    "A full-earth ground magnetic perturbation forecasting model using deep learning\n",
    "\n",
    "In this notebook, we shall see how to forecast ground magnetic perturbation across the northern hemisphere using solar wind measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea01cf",
   "metadata": {},
   "source": [
    "Authors: Vishal Upendran, Panos Tigas, Bashi Ferdousi, Teo Bloch, Mark Cheung, Siddha Ganju, Asti Bhat, Ryan McGranaghan, Yarin Gal\n",
    "\n",
    "\n",
    "**Cite as:** Tigas, P., Bloch, T., Upendran, V., Ferdoushi, B., Cheung, M., Ganju, S., McGranaghan, R.M., Gal, Y. and Bhatt, A., 2021. Global Earth Magnetic Field Modeling and Forecasting with Spherical Harmonics Decomposition. arXiv preprint [arXiv:2102.01447](https://arxiv.org/abs/2102.01447)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d0cde",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Ground magnetic field perturbation is currently estimated based on Magnetic Hydrodynamic (MHD) and empirical models. While useful, MHD models are computationally expensive for high-resolution models that are required for small-scale perturbation, and empirical models do not provide a dynamic forecast. \n",
    "\n",
    "In this study, we use Spherical Harmonics (SH) to create high-resolution global models of the northward ($\\delta\\mathrm{B}_\\mathrm{N}$) and Eastward ($\\delta\\mathrm{B}_\\mathrm{E}$) components of the perturbation with a lead time of 30 min. How do we do this?\n",
    "\n",
    "We summarize 2-hours of solar wind conditions measured at L1 point (from the OMNI dataset) using Recurrent Neural Networks (RNNs) to generate a summary \"hidden state\" of the solar wind measurements. This state is then passed to a fully-connected layer of Multi Layer Perceptro (MLP) to generate a vector of coefficients. These coefficients are then contracted with a basis of spherical harmonics which is generated every forward pass depending on the locations of the different stations which have measurements. The output after the contraction operation is a set of ($\\delta\\mathrm{B}_\\mathrm{N}$) and ($\\delta\\mathrm{B}_\\mathrm{E}$) measurements at each station location.\n",
    "\n",
    "In this notebook, we shall generate the forecast for the 2015 storm dataset, and visualize the global predictions with the targets as a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7a779",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Notebook setup\n",
    "2. Download and read data\n",
    "3. Setup model\n",
    "4. Generate forecast\n",
    "5. Time series of forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c084d",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "We first need to clone the repo and install packages. Then, we will need to import them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ccc76",
   "metadata": {},
   "source": [
    "You will mostly be asked to restart runtime after installation. In such a case, you don't need to run this cell after the packages have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d769f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ptigas/geoeffectivenet.git\n",
    "%cd geoeffectivenet/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16571c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.10.0 torch==1.9.0 pytorch-lightning==1.3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9152a6",
   "metadata": {},
   "source": [
    "⚠️ Restart the runtime, and run from the cell labelled \"cell 3\" below ⚠️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "\n",
    "if torch.cuda.is_available() == False:\n",
    "    print(\n",
    "        f'CUDA is unavailable. If you are running this notebook on Colab, go to Runtime > Change runtime type, and set \"GPU\"'\n",
    "    )\n",
    "else:\n",
    "    print(\"CUDA is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1715b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'geoeffectivenet/'\n",
      "/Users/pwright/Documents/personal/SpaceML/helionb-geoeff/notebooks/01_geoeff_2020\n"
     ]
    }
   ],
   "source": [
    "%cd geoeffectivenet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e100a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import h5py\n",
    "from astropy.time import Time\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851eac65",
   "metadata": {
    "id": "8-0ZEES36eWR"
   },
   "source": [
    "## Helper functions \n",
    "\n",
    "Define some important helper functions to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ed014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataloader import SuperMAGIAGADataset,OMNIDataset,ShpericalHarmonicsDatasetBucketized\n",
    "from utils.data_utils import get_omni_data,get_iaga_data_as_list\n",
    "from utils.splitter import generate_indices\n",
    "from utils.plot import SqueezedNorm\n",
    "from Benchmark.metricgen import EventMetrics,Generate_metrics\n",
    "from Benchmark.Forecaster import Forecaster,Generate_complete_weimer_forecast,Generate_weimer_forecast\n",
    "from models.geoeffectivenet import *\n",
    "import pickle\n",
    "torch.set_default_dtype(torch.float64)  # this is important else it will overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = {'NeuralRNNWiemer_HidddenSuperMAG':NeuralRNNWiemer_HidddenSuperMAG,\n",
    "        'NeuralRNNWiemer':NeuralRNNWiemer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524353d",
   "metadata": {},
   "source": [
    "## Download and read data\n",
    "\n",
    "The first step is to download the data, download the checkpoints, and download the code repository. The dataset contains two storm times -- 2011 storm and 2015 storm. These are standard storms which are used for evaluation of geomagnetic perturbation forecasting models.\n",
    "\n",
    "The data, along with the model checkpoints are made available in a Google cloud bucket. The code has been made open source in a github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ab1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to download data from the bucket\n",
    "!mkdir -p Storm\n",
    "!gsutil cp -r gs://storm_subset/ Storm/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43a16b",
   "metadata": {},
   "source": [
    "## Load hyperparameters\n",
    "\n",
    "Hyperparameters are free parameters that are non-trainable, and  are fixed for a particular task. These values **must not** be changed during inference time, but we can make different realizations of the model with different values assigned to these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_length = 1 #WAS DEFAULT IN THE CODE\n",
    "past_omni_length = 120\n",
    "nmax = 20\n",
    "targets = [\"dbe_nez\", \"dbn_nez\"]\n",
    "lag = 30\n",
    "learning_rate = 5e-3\n",
    "batch_size = 500\n",
    "omni_resolution = 1\n",
    "dropout_prob = 0.7\n",
    "l2reg = 5e-5\n",
    "n_hidden = 8\n",
    "loss = 'MAE'\n",
    "modname = \"NeuralRNNWiemer\"\n",
    "basepath = \"Storm/storm_subset/model/Best/\"\n",
    "# load scaler to unstandardize data\n",
    "scalers = pickle.load(open(f\"{basepath}scalers.p\", \"rb\"))\n",
    "_mean, _std = scalers[\"supermag\"]\n",
    "\n",
    "dbe_mean, dbn_mean = _mean\n",
    "dbe_std, dbn_std = _std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac705868",
   "metadata": {},
   "source": [
    "### Load the data and the storm indices\n",
    "\n",
    "As mentioned earlier, we have two datasets - years 2011 and 2015. Select any of the two years in the cell below, and run the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2011\n",
    "dataset = np.load(f\"Storm/storm_subset/{YEAR}/supermag_omni_data.npz\")\n",
    "supermag_data = SuperMAGIAGADataset(dataset['dates'],dataset['data'],dataset['features'])\n",
    "omni_data = OMNIDataset(pd.DataFrame(data=dataset['omni'],columns=dataset['omni_features']))\n",
    "storm_idx = dataset['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16801c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_dataset = ShpericalHarmonicsDatasetBucketized(supermag_data,omni_data,storm_idx,\n",
    "        f107_dataset=f\"Storm/storm_subset/{YEAR}/f107.npz\",targets=targets,past_omni_length=past_omni_length,\n",
    "        past_supermag_length=1,future_length=future_length,lag=lag,zero_omni=False,\n",
    "        zero_supermag=False,scaler=scalers,training_batch=False,nmax=nmax,inference=True)\n",
    "storm_loader = data.DataLoader(storm_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_idx = [np.where(storm_dataset.supermag_features == target)[0][0] for target in targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e576995",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8b470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIHccku3iQCn",
    "outputId": "84d4d2d4-1c87-497f-ee44-86ad6e845376"
   },
   "outputs": [],
   "source": [
    "NN_md = md[modname]\n",
    "model = NN_md.load_from_checkpoint(f'{basepath}/epoch=8-step=3077.ckpt', strict=False, \n",
    "                                                 past_omni_length=past_omni_length,\n",
    "                                                 future_length=future_length,\n",
    "                                                 supermag_features=storm_dataset.supermag_features,\n",
    "                                                 omni_features=storm_dataset.omni_features,\n",
    "                                                 nmax=nmax,omni_resolution=omni_resolution,\n",
    "                                                 targets_idx=targets_idx,learning_rate = learning_rate,\n",
    "                                                 l2reg=l2reg,\n",
    "                                                 dropout_prob=dropout_prob,\n",
    "                                                 n_hidden=n_hidden,\n",
    "                                                 loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d26959",
   "metadata": {
    "id": "Vwi4B5j2m3fm"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda().double()\n",
    "else:\n",
    "    model = model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6696ec2",
   "metadata": {},
   "source": [
    "## Generate forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbe_index = np.where(storm_dataset.supermag_features == 'dbe_nez')[0][0]\n",
    "dbn_index = np.where(storm_dataset.supermag_features == 'dbn_nez')[0][0]\n",
    "print(dbn_index,dbe_index)\n",
    "target_index = {'dbe':dbe_index,'dbn':dbn_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c494a5",
   "metadata": {
    "id": "J_ah54aPiR2t"
   },
   "outputs": [],
   "source": [
    "Predictions,Targets,All_times_coeff,Date_arr,MLT_sup_all,Mcolat_sup_all = Forecaster(storm_loader,model,\n",
    "                                                                                     dbe_mean,dbe_std,dbn_mean,\n",
    "                                                                                     dbn_std,target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef4908",
   "metadata": {},
   "source": [
    "## Time series of forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date_arr['dbe'].shape,Predictions['dbe'].shape,Targets['dbe'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c59d9",
   "metadata": {},
   "source": [
    "### How does the forecast for one (any random) station look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_index = 100\n",
    "plt.style.use('default')\n",
    "fig,ax = plt.subplots(2,1,figsize=(12,8))\n",
    "\n",
    "ax[0].plot(Predictions['dbe'][:,station_index],'orange',label='FDL')\n",
    "ax[0].plot(Targets['dbe'][:,station_index],'k',label='Target')\n",
    "ax[0].set_ylabel(\"$\\\\delta \\mathrm{B}_{\\mathrm{E}}$ (nT)\")\n",
    "ax[0].set_xlabel(\"Time from the start (min)\")\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title(f\"Forecast for station: {dataset['stations'][station_index]}\",fontsize=20)\n",
    "\n",
    "ax[1].plot(Predictions['dbn'][:,station_index],'orange',label='FDL')\n",
    "ax[1].plot(Targets['dbn'][:,station_index],'k',label='Target')\n",
    "ax[1].set_ylabel(\"$\\\\delta \\mathrm{B}_{\\mathrm{N}}$ (nT)\")\n",
    "ax[1].set_xlabel(\"Time from the start (min)\")\n",
    "ax[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2f402",
   "metadata": {},
   "source": [
    "**Figure 1**: A comparison of measured (black) and forecasted (orange) $\\delta\\mathrm{B}_{\\mathrm{E}}$ (top) and $\\delta\\mathrm{B}_{\\mathrm{N}}$ (bottom) for a given station.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0f34b",
   "metadata": {},
   "source": [
    "### How does the joint distribution of forecast and targets look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "bad_indices = np.isnan(Targets['dbn']) | np.isnan(Predictions['dbn'])\n",
    "im = ax[0].hist2d(Targets['dbn'][~bad_indices],Predictions['dbn'][~bad_indices],bins=150,cmap='cividis',norm=mpl.colors.LogNorm())\n",
    "ax[0].set_ylabel(\"Predicted\",fontsize=15)\n",
    "ax[0].set_xlabel(\"Target\",fontsize=15)\n",
    "ax[0].set_xlim([np.nanmin(Predictions['dbn'].ravel()),np.nanmax(Targets['dbn'].ravel())+0.5])\n",
    "ax[0].set_ylim(ax[0].get_xlim())\n",
    "limy=ax[0].get_ylim()[0]#0.94\n",
    "limx=ax[0].get_xlim()[-1]-0.1#0.99\n",
    "ax[0].set_title(\"$\\delta \\mathrm{b}_{\\mathrm{N}}$\")\n",
    "cbar = fig.colorbar(im[3], ax=ax[0], orientation='vertical')\n",
    "cbar.ax.set_title(\"#\",fontsize=15)\n",
    "limy=ax[0].get_ylim()\n",
    "limx=ax[0].get_xlim()\n",
    "x_1 = np.linspace(limx[0],limx[1],4)\n",
    "y_1 = x_1-limx[0]+limy[0]\n",
    "ax[0].plot(x_1,y_1,'k')\n",
    "\n",
    "\n",
    "bad_indices = np.isnan(Targets['dbe']) | np.isnan(Predictions['dbe'])\n",
    "im  = ax[1].hist2d(Targets['dbe'][~bad_indices],Predictions['dbe'][~bad_indices],bins=150,cmap='cividis',norm=mpl.colors.LogNorm())\n",
    "ax[1].set_ylabel(\"Predicted\",fontsize=15)\n",
    "ax[1].set_xlabel(\"Target\",fontsize=15)\n",
    "ax[1].set_xlim([np.nanmin(Predictions['dbe'].ravel()),np.nanpercentile(Targets['dbe'].ravel(),99.8)+0.5])\n",
    "ax[1].set_ylim(ax[1].get_xlim())\n",
    "limy=ax[1].get_ylim()[0]#0.94\n",
    "limx=ax[1].get_xlim()[-1]-0.1#0.99\n",
    "ax[1].set_title(\"$\\delta \\mathrm{b}_{\\mathrm{E}}$\")\n",
    "cbar = fig.colorbar(im[3], ax=ax[1], orientation='vertical')\n",
    "cbar.ax.set_title(\"#\",fontsize=15)\n",
    "limy=ax[1].get_ylim()\n",
    "limx=ax[1].get_xlim()\n",
    "x_1 = np.linspace(limx[0],limx[1],4)\n",
    "y_1 = x_1-limx[0]+limy[0]\n",
    "ax[1].plot(x_1,y_1,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f3da0",
   "metadata": {},
   "source": [
    "**Figure 2**: Joint histogram of measured and forecasted $\\delta\\mathrm{B}_{\\mathrm{N}}$ (left) and $\\delta\\mathrm{B}_{\\mathrm{E}}$ (right) for a given station.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083f1e6",
   "metadata": {},
   "source": [
    "### Forecast for the top-3 best and worst performing stations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_station(dt,fdl,targ,ax,**kwargs):\n",
    "    l1 = ax.plot(dt,fdl,c=\"#00c5ff\",label='FDL')\n",
    "    ax.yaxis.label.set_color(\"#00c5ff\")\n",
    "    ax_tmp = ax.twinx()\n",
    "    l2 = ax_tmp.plot(dt,targ,c=\"#181820\",label=f\"{kwargs['stat']}\")\n",
    "    ax.set_ylabel(\"$\\\\delta \\mathrm{b}_{\\mathrm{H}}$ (nT)\",fontsize=15)\n",
    "    ax_tmp.set_ylabel(\"$\\\\delta \\mathrm{b}_{\\mathrm{H}}$ (nT)\",fontsize=15)\n",
    "    ax.set_title(f\"{kwargs['metric']} = {kwargs['mval']:.2f} nT\")\n",
    "    lns = l1+l2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax.spines['left'].set_color(\"#00c5ff\")\n",
    "    ax.legend(lns, labs, loc='best')\n",
    "    ax.xaxis.get_majorticklabels()\n",
    "    ax.tick_params(axis='x', rotation=-45)\n",
    "    limy=ax.get_ylim()[-1]#0.94\n",
    "    limx=ax.get_xlim()[0]-0.1#0.99\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbh_pred.shape,dbh_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_dt = pd.to_datetime(Date_arr['dbe'].ravel(),unit='s')\n",
    "symh_value = dataset['symh']\n",
    "stations = dataset['stations']\n",
    "\n",
    "dbh_pred = np.sqrt(Predictions['dbe']**2+Predictions['dbn']**2)\n",
    "dbh_targ = np.sqrt(Targets['dbe']**2+Targets['dbn']**2)\n",
    "\n",
    "subset = ~np.isnan(dbh_targ).any(axis=0)[:len(stations)]\n",
    "dbh_pred = dbh_pred[:,:len(stations)][:,subset]\n",
    "dbh_targ = dbh_targ[:,:len(stations)][:,subset]\n",
    "stations = stations[subset]\n",
    "\n",
    "mae = np.nanmean(np.abs(dbh_pred-dbh_targ),axis=0)\n",
    "notnan = ~np.isnan(mae) \n",
    "sort_mae = np.argsort(mae[notnan])\n",
    "\n",
    "mae = mae[notnan]\n",
    "dbh_pred = dbh_pred[:,notnan]\n",
    "dbh_targ = dbh_targ[:,notnan]\n",
    "stations = stations[notnan]\n",
    "\n",
    "i_bz = np.where(storm_dataset.omni_features=='bz')\n",
    "i_s,i_e = storm_dataset.sg_indices[0,-1],storm_dataset.sg_indices[-1,-1]+1\n",
    "bz = storm_dataset.omni[i_s:i_e,i_bz]*scalers['omni'][1][i_bz]+scalers['omni'][0][i_bz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig,ax = plt.subplots(4,2,figsize=(12,12))\n",
    "\n",
    "ax[0,0].plot(pl_dt,bz.ravel(),c=\"#181820\")\n",
    "ax[0,0].set_ylabel(\"IMF $\\mathrm{b}_{\\mathrm{z}}$ (nT)\",fontsize=15)\n",
    "ax[0,0].xaxis.get_majorticklabels()\n",
    "ax[0,0].tick_params(axis='x', rotation=-45)\n",
    "limy=ax[0,0].get_ylim()[-1]#0.94\n",
    "limx=ax[0,0].get_xlim()[0]-0.1#0.99\n",
    "    \n",
    "ax[0,1].plot(pl_dt,symh_value.ravel(),c=\"#181820\")\n",
    "ax[0,1].set_ylabel(\"Sym-H (nT)\",fontsize=15)\n",
    "ax[0,1].xaxis.get_majorticklabels()\n",
    "ax[0,1].tick_params(axis='x', rotation=-45)\n",
    "limy=ax[0,1].get_ylim()[-1]#0.94\n",
    "limx=ax[0,1].get_xlim()[0]-0.1#0.99\n",
    "\n",
    "for i in np.arange(3):\n",
    "    plot_station(pl_dt,dbh_pred[:,sort_mae[i]],dbh_targ[:,sort_mae[i]],ax[i+1,0],\n",
    "                 metric='MAE',mval=mae[sort_mae[i]],pno=2*i,stat=stations[sort_mae[i]])\n",
    "for i in np.arange(3):\n",
    "    plot_station(pl_dt,dbh_pred[:,sort_mae[-i-1]],dbh_targ[:,sort_mae[-i-1]],ax[i+1,1],\n",
    "                 metric='MAE',mval=mae[sort_mae[-i-1]],pno=2*i+1,stat=stations[sort_mae[-i-1]])\n",
    "fig.suptitle(f\"{YEAR} storm: Comparison of performance for the top - 3 best and worst performing stations\",fontsize=15)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e06fa",
   "metadata": {},
   "source": [
    "**Figure 3**: The IMF Bz (left), Sym-H (right) in the first rwo, and top 3 best (left column) and worst (right columns) performing stations in the subsequent rows. The blue colour indicates forecast from our model, while the black colour indicates measurements at different stations (in the legend of each figure), with the MAE reported on top.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231ae43",
   "metadata": {},
   "source": [
    "## Forecast all over the Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b6711",
   "metadata": {},
   "source": [
    "### Generate latitude-longitude grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLT_sup_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d05bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mlt = np.linspace(np.nanmin(MLT_sup_all['dbe']),np.nanmax(MLT_sup_all['dbe']),100)\n",
    "_colat = np.linspace(np.nanmin(Mcolat_sup_all['dbe']),np.nanmax(Mcolat_sup_all['dbe']),360)\n",
    "_mlt,_colat = np.meshgrid(_mlt,_colat)\n",
    "_basis = basis_matrix(nmax ,_mlt, _colat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c75eae",
   "metadata": {},
   "source": [
    "### Transform coefficients to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca37056",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdl_on_grid_gif = {'dbe':[],'dbn':[]}\n",
    "\n",
    "fdl_on_grid_gif['dbn'] = np.einsum('bij,lj->bil',_basis,All_times_coeff['dbn'])\n",
    "fdl_on_grid_gif['dbe'] = np.einsum('bij,lj->bil',_basis,All_times_coeff['dbe'])\n",
    "\n",
    "fdl_on_grid_gif['dbn'] = fdl_on_grid_gif['dbn']*dbn_std + dbn_mean\n",
    "fdl_on_grid_gif['dbe'] = fdl_on_grid_gif['dbe']*dbe_std + dbe_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f08608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check.\n",
    "print(fdl_on_grid_gif['dbn'].shape,fdl_on_grid_gif['dbe'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b4e01",
   "metadata": {},
   "source": [
    "## What does the global forecast look like?\n",
    "\n",
    "We can plot the forecast and measurements across the globe ==> get global maps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'dbn'\n",
    "f = 1\n",
    "fig1 = plt.figure(figsize=(12*f,8*f),dpi=100)\n",
    "gs = fig1.add_gridspec(8, 2)\n",
    "\n",
    "theta = _mlt \n",
    "phi = _colat \n",
    "cmap=cm.get_cmap(\"RdYlBu_r\",17)\n",
    "ax1 = fig1.add_subplot(gs[:-1,0], projection='polar')\n",
    "ax1.set_theta_offset(-np.pi/2)\n",
    "ax2 = fig1.add_subplot(gs[:-1,1], projection='polar')\n",
    "ax2.set_theta_offset(-np.pi/2)\n",
    "# ax3 = fig1.add_subplot(gs[:-1,2], projection='polar')\n",
    "# ax3.set_theta_offset(-np.pi/2)\n",
    "cax = fig1.add_subplot(gs[-1,:])\n",
    "time_ind=1500\n",
    "d = storm_dataset.dates[time_ind]\n",
    "cax.cla()\n",
    "dt = pd.to_datetime(d,unit='s')\n",
    "\n",
    "dbH2 = fdl_on_grid_gif[k][...,time_ind] #np.sqrt(np.square(Targets['dbe'][:,:,start:end+1])+np.square(Targets['dbn'][:,:,start:end+1]))\n",
    "\n",
    "maxval = np.nanpercentile(dbH2,95)\n",
    "minval = np.nanpercentile(dbH2,5)\n",
    "norm=SqueezedNorm(vmin=minval, vmax=maxval, mid=np.nanmean(dbH2), s1=2, s2=2)\n",
    "\n",
    "dbH = fdl_on_grid_gif[k][...,time_ind] #np.sqrt(np.square(fdl_on_weimergrid_gif['dbe'][:,:,start:end+1][...,time_ind])+np.square(fdl_on_weimergrid_gif['dbn'][:,:,start:end+1][...,time_ind]))\n",
    "c2=ax2.pcolormesh(theta,phi, dbH,cmap=cmap,norm=norm)  \n",
    "ax2.set_ylim([np.nanmin(Mcolat_sup_all[k][time_ind]),np.nanmax(Mcolat_sup_all[k][time_ind])])\n",
    "ax2.grid(linewidth=1,color='white',linestyle='--')\n",
    "ax2.text(3.5,1.1,f\"FDL 30-min forecast\",fontsize=15*f)\n",
    "\n",
    "dbH = Targets[k][time_ind] #np.sqrt(np.square(Targets['dbe'][start:end+1][time_ind])+np.square(Targets['dbn'][start:end+1][time_ind]))\n",
    "c3=ax1.scatter(MLT_sup_all[k][time_ind],Mcolat_sup_all[k][time_ind], c=dbH,cmap=cmap,norm=norm,s=36*f)\n",
    "ax1.text(3.45,1.16,f\"Target SuperMag\",fontsize=15*f)\n",
    "ax1.grid(linewidth=2,color='white')\n",
    "fig1.colorbar(c3, cax=cax,orientation='horizontal')\n",
    "cax.set_xlabel(r'Northward magnetic field perturbation ($\\delta\\mathrm{B}_{\\mathrm{N}}$) (nT)',fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3199ef",
   "metadata": {},
   "source": [
    "**Figure 4**: Global maps of measurement (left) and forecast (right).\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdf240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06eac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'dbn'\n",
    "f = 1\n",
    "fig1 = plt.figure(figsize=(12*f,8*f),dpi=100)\n",
    "gs = fig1.add_gridspec(8, 2)\n",
    "\n",
    "camera = Camera(fig1)\n",
    "theta = _mlt \n",
    "phi = _colat \n",
    "cmap=cm.get_cmap(\"RdYlBu_r\",17)\n",
    "ax1 = fig1.add_subplot(gs[:-1,0], projection='polar')\n",
    "ax1.set_theta_offset(-np.pi/2)\n",
    "ax2 = fig1.add_subplot(gs[:-1,1], projection='polar')\n",
    "ax2.set_theta_offset(-np.pi/2)\n",
    "# ax3 = fig1.add_subplot(gs[:-1,2], projection='polar')\n",
    "# ax3.set_theta_offset(-np.pi/2)\n",
    "cax = fig1.add_subplot(gs[-1,:])\n",
    "\n",
    "for time_ind,d in enumerate(storm_dataset.dates[1500:][::12]):\n",
    "    cax.cla()\n",
    "    dt = pd.to_datetime(d,unit='s')\n",
    "    \n",
    "    dbH2 = fdl_on_grid_gif[k][...,time_ind] #np.sqrt(np.square(Targets['dbe'][:,:,start:end+1])+np.square(Targets['dbn'][:,:,start:end+1]))\n",
    "\n",
    "    maxval = np.nanpercentile(dbH2,95)\n",
    "    minval = np.nanpercentile(dbH2,5)\n",
    "    norm=SqueezedNorm(vmin=minval, vmax=maxval, mid=np.nanmean(dbH2), s1=2, s2=2)\n",
    "\n",
    "    dbH = fdl_on_grid_gif[k][...,time_ind] #np.sqrt(np.square(fdl_on_weimergrid_gif['dbe'][:,:,start:end+1][...,time_ind])+np.square(fdl_on_weimergrid_gif['dbn'][:,:,start:end+1][...,time_ind]))\n",
    "    c2=ax2.pcolormesh(theta,phi, dbH,cmap=cmap,norm=norm)  \n",
    "    ax2.set_ylim([np.nanmin(Mcolat_sup_all[k][time_ind]),np.nanmax(Mcolat_sup_all[k][time_ind])])\n",
    "    ax2.grid(linewidth=1,color='white',linestyle='--')\n",
    "    ax2.text(3.5,1.1,f\"FDL 30-min forecast\",fontsize=15*f)\n",
    "\n",
    "    dbH = Targets[k][time_ind] #np.sqrt(np.square(Targets['dbe'][start:end+1][time_ind])+np.square(Targets['dbn'][start:end+1][time_ind]))\n",
    "    c3=ax1.scatter(MLT_sup_all[k][time_ind],Mcolat_sup_all[k][time_ind], c=dbH,cmap=cmap,norm=norm,s=36*f)\n",
    "    ax1.text(3.45,1.16,f\"Target SuperMag\",fontsize=15*f)\n",
    "    ax1.grid(linewidth=2,color='white')\n",
    "    fig1.colorbar(c3, cax=cax,orientation='horizontal')\n",
    "    cax.set_xlabel(r'Northward magnetic field perturbation ($\\delta\\mathrm{B}_{\\mathrm{N}}$) (nT)',fontsize=15)\n",
    "    camera.snap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04539e20",
   "metadata": {},
   "source": [
    "**Figure 5**: Global maps of $\\delta\\mathrm{B}_{\\mathrm{N}}$measurement (left) and forecast (right).\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480288e",
   "metadata": {},
   "source": [
    "#### If you prefer saving the animation as a video, use `animation.save()`. Otherwise, run the other cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = camera.animate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721601ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#animation.save('dbn.mp4',bitrate=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18baeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'dbe'\n",
    "f = 1\n",
    "fig1 = plt.figure(figsize=(12*f,8*f),dpi=100)\n",
    "gs = fig1.add_gridspec(8, 2)\n",
    "\n",
    "camera = Camera(fig1)\n",
    "theta = _mlt \n",
    "phi = _colat \n",
    "cmap=cm.get_cmap(\"RdYlBu_r\",17)\n",
    "ax1 = fig1.add_subplot(gs[:-1,0], projection='polar')\n",
    "ax1.set_theta_offset(-np.pi/2)\n",
    "ax2 = fig1.add_subplot(gs[:-1,1], projection='polar')\n",
    "ax2.set_theta_offset(-np.pi/2)\n",
    "# ax3 = fig1.add_subplot(gs[:-1,2], projection='polar')\n",
    "# ax3.set_theta_offset(-np.pi/2)\n",
    "cax = fig1.add_subplot(gs[-1,:])\n",
    "\n",
    "for time_ind,d in enumerate(storm_dataset.dates[1500:][::12]):\n",
    "    cax.cla()\n",
    "    dt = pd.to_datetime(d,unit='s')\n",
    "    \n",
    "    dbH2 = fdl_on_grid_gif[k][...,time_ind] #np.sqrt(np.square(Targets['dbe'][:,:,start:end+1])+np.square(Targets['dbn'][:,:,start:end+1]))\n",
    "\n",
    "    maxval = np.nanpercentile(dbH2,95)\n",
    "    minval = np.nanpercentile(dbH2,5)\n",
    "    norm=SqueezedNorm(vmin=minval, vmax=maxval, mid=np.nanmean(dbH2), s1=3, s2=3)\n",
    "\n",
    "    dbH = fdl_on_grid_gif[k][...,time_ind] #np.sqrt(np.square(fdl_on_weimergrid_gif['dbe'][:,:,start:end+1][...,time_ind])+np.square(fdl_on_weimergrid_gif['dbn'][:,:,start:end+1][...,time_ind]))\n",
    "    c2=ax2.pcolormesh(theta,phi, dbH,cmap=cmap,norm=norm)  \n",
    "    ax2.set_ylim([np.nanmin(Mcolat_sup_all[k][time_ind]),np.nanmax(Mcolat_sup_all[k][time_ind])])\n",
    "    ax2.grid(linewidth=1,color='white',linestyle='--')\n",
    "    ax2.text(3.5,1.1,f\"FDL 30-min forecast\",fontsize=15*f)\n",
    "\n",
    "    dbH = Targets[k][time_ind] #np.sqrt(np.square(Targets['dbe'][start:end+1][time_ind])+np.square(Targets['dbn'][start:end+1][time_ind]))\n",
    "    c3=ax1.scatter(MLT_sup_all[k][time_ind],Mcolat_sup_all[k][time_ind], c=dbH,cmap=cmap,norm=norm,s=36*f)\n",
    "    ax1.text(3.45,1.16,f\"Target SuperMag\",fontsize=15*f)\n",
    "    ax1.grid(linewidth=2,color='white')\n",
    "    fig1.colorbar(c3, cax=cax,orientation='horizontal')\n",
    "    cax.set_xlabel(r'Eastward magnetic field perturbation ($\\delta\\mathrm{B}_{\\mathrm{E}}$) (nT)',fontsize=15)\n",
    "    camera.snap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cee2ac",
   "metadata": {},
   "source": [
    "**Figure 6**: Global maps of $\\delta\\mathrm{B}_{\\mathrm{E}}$measurement (left) and forecast (right).\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29713cac",
   "metadata": {},
   "source": [
    "#### If you prefer saving the animation as a video, use `animation.save()`. Otherwise, run the other cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = camera.animate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation.save('dbe.mp4',bitrate=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca951a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(animation.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
